{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Team 27\n",
    "#Κινους Βασιλειος Αλεξανδρος\n",
    "#8834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries that will be needed\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,normalize\n",
    "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "import hyperopt\n",
    "from hyperopt import hp\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import  accuracy_score, f1_score,precision_score,recall_score\n",
    "import time\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e03076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('C:/Users/alexk/Downloads/datasetC.csv',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the target column\n",
    "\n",
    "df = df.rename(columns={400: \"Class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target column counts (slight imbalance)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.set_style(\"darkgrid\")\n",
    "df[\"Class\"].value_counts().plot.bar(rot=0)\n",
    "plt.title(\"Value counts of Class column\",fontsize = 20)\n",
    "plt.xlabel(\"Class\",fontsize = 12)\n",
    "plt.ylabel(\"Count\",fontsize = 12)\n",
    "plt.savefig(\"C:/Users/alexk/Desktop/protipa/valuecountsexercise4.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24eded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083179d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y==1] = 0 \n",
    "y[y==2] = 1 \n",
    "y[y==3] = 2 \n",
    "y[y==4] = 3 \n",
    "y[y==5] = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f92be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an explained variance ratio plot to check if pca is meaningful\n",
    "\n",
    "pca_ratios = []\n",
    "for i in range(300):\n",
    "    #define PCA model to use\n",
    "    pca = PCA(n_components=i,random_state = 77)\n",
    "\n",
    "    #fit PCA model to data\n",
    "    pca_fit = pca.fit_transform(X)\n",
    "    \n",
    "    pca_ratios.append(pca.explained_variance_ratio_.sum())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(8,8))    \n",
    "\n",
    "closest_value = min(pca_ratios, key=lambda x: abs(x - 0.8))\n",
    "index_of_closest_value = pca_ratios.index(closest_value)\n",
    "\n",
    "\n",
    "g = sns.lineplot(data=pca_ratios)\n",
    "g.axhline(0.8,ls='--',c=\"red\")\n",
    "g.axvline(index_of_closest_value,ls='--',c=\"red\")\n",
    "\n",
    "\n",
    "plt.scatter(index_of_closest_value, closest_value, color='red', zorder=5)\n",
    "\n",
    "plt.xlabel(\"Components\",fontsize=12)\n",
    "plt.ylabel(\"Explained Variance\",fontsize=12)\n",
    "plt.title(\"Explained variance of different number of components\",fontsize=18)\n",
    "plt.savefig(\"C:/Users/alexk/Desktop/protipa/explainedvariancesum80.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Ctest has 1000 rows which is 20% of DatasetC so split 0.2% \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51525dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets try with a knn classifier with a tuning on its neighbors number.\n",
    "\n",
    "knn_predictions = []\n",
    "accuracy_matrix = []\n",
    "knn_classifier = []\n",
    "for neighbors in range(1,16):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    \n",
    "    knn_classifier.append(knn)\n",
    "    \n",
    "    knn_predictions.append(y_pred_knn)\n",
    "    \n",
    "    accuracy_matrix.append(accuracy_score(y_test, y_pred_knn))\n",
    "    \n",
    "plt.figure(figsize = (8,8))\n",
    "g = sns.lineplot(data=accuracy_matrix,marker=\"o\",markersize = 12)\n",
    "g.set_xticks(range(len(accuracy_matrix)))\n",
    "g.set_xticklabels(np.arange(1,16),fontsize = 12)\n",
    "plt.title(\"Accuracy of different neighbors\",fontsize = 20)\n",
    "\n",
    "plt.scatter(accuracy_matrix.index(max(accuracy_matrix)),max(accuracy_matrix), color='red', zorder=5)\n",
    "\n",
    "#region of interest for the exercise because there are more points created to show the accuracy drop\n",
    "#plt.axvspan(0, 9, color='blue', alpha=0.2)\n",
    "\n",
    "for i in range(15) :\n",
    "\n",
    "    g.text(i+0.12,accuracy_matrix[i]-0.0015,\"N ={}\".format(i+1))\n",
    "    \n",
    "plt.savefig(\"C:/Users/alexk/Desktop/protipa/knnaccuraciesexercise4\", bbox_inches='tight')    \n",
    "plt.show()\n",
    "\n",
    "print(\"Best achieved accuracy with knn classifier is {}% with N-Neighbors = {}\".format(100*(max(accuracy_matrix)),accuracy_matrix.index(max(accuracy_matrix))+1))\n",
    "\n",
    "best_preds_knn = knn_predictions[accuracy_matrix.index(max(accuracy_matrix))]\n",
    "\n",
    "print('Knn with N-Neighbors = {} model accuracy score: {}'.format(accuracy_matrix.index(max(accuracy_matrix))+1,accuracy_score(y_test, best_preds_knn)))\n",
    "print('Knn with N-Neighbors = {} model precision score: {}'.format(accuracy_matrix.index(max(accuracy_matrix))+1,precision_score(y_test, best_preds_knn,average=\"weighted\")))\n",
    "print('Knn with N-Neighbors = {} model recall score: {}'.format(accuracy_matrix.index(max(accuracy_matrix))+1,recall_score(y_test, best_preds_knn,average=\"weighted\")))\n",
    "print('Knn with N-Neighbors = {} model f1 score: {}'.format(accuracy_matrix.index(max(accuracy_matrix))+1,f1_score(y_test, best_preds_knn,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#now try svm rbf kernel tuned\n",
    "\n",
    "C_range_ = [1, 1e2,1e3]\n",
    "gamma_range_ = [1e-2,1e-1, 1]\n",
    "svm_classifiers = []\n",
    "svm_predictions = []\n",
    "accuracy_matrix_svm = []\n",
    "for C in C_range_:\n",
    "    for gamma in gamma_range_:\n",
    "        svm_rbf = SVC(C=C, gamma=gamma)\n",
    "        svm_rbf.fit(X_train, y_train)\n",
    "        svm_classifiers.append(svm_rbf)\n",
    "        \n",
    "        y_pred_svm = svm_rbf.predict(X_test)\n",
    "    \n",
    "        \n",
    "    \n",
    "        svm_predictions.append(y_pred_svm)\n",
    "    \n",
    "        accuracy_matrix_svm.append(accuracy_score(y_test, y_pred_svm))\n",
    "        \n",
    "plt.figure(figsize = (8,8))\n",
    "g = sns.lineplot(data=accuracy_matrix_svm,marker=\"o\",markersize = 12)\n",
    "g.set_xticks(range(len(accuracy_matrix_svm)))\n",
    "g.set_xticklabels(np.arange(1,10),fontsize = 12)\n",
    "plt.title(\"Accuracy of different C and gamma combinations\",fontsize = 20)\n",
    "\n",
    "plt.scatter(accuracy_matrix_svm.index(max(accuracy_matrix_svm)),max(accuracy_matrix_svm), color='red', zorder=5)\n",
    "\n",
    "for i in range(9) :\n",
    "    \n",
    "    C_ = int(svm_classifiers[i].get_params()[\"C\"])\n",
    "    gamma_ = svm_classifiers[i].get_params()[\"gamma\"]\n",
    "    \n",
    "    g.text(i+0.14,accuracy_matrix_svm[i]-0.007,\"C ={}\\nG ={}\".format(C_,gamma_))\n",
    "\n",
    "plt.savefig(\"C:/Users/alexk/Desktop/protipa/svmaccuraciesexercise4\", bbox_inches='tight') \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print(\"Best achieved accuracy with tuned rbf kernel svm is {} with C ={} and gamma ={}\".format(max(accuracy_matrix_svm),(svm_classifiers[accuracy_matrix_svm.index(max(accuracy_matrix_svm))].get_params()[\"C\"]),(svm_classifiers[accuracy_matrix_svm.index(max(accuracy_matrix_svm))].get_params()[\"gamma\"])))\n",
    "\n",
    "best_preds_svm = svm_predictions[accuracy_matrix_svm.index(max(accuracy_matrix_svm))]\n",
    "\n",
    "print('SVM best model accuracy score: {}'.format(accuracy_score(y_test, best_preds_svm)))\n",
    "print('SVM best model precision score: {}'.format(precision_score(y_test, best_preds_svm,average=\"weighted\")))\n",
    "print('SVM best model recall score: {}'.format(recall_score(y_test, best_preds_svm,average=\"weighted\")))\n",
    "print('SVM best model f1 score: {}'.format(f1_score(y_test, best_preds_svm,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f093200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepwise Hyperopt tuning of xgboost with nested crossvalidation\n",
    "\n",
    "#Keep n iterations constant and learning rate at 0.1 and experiment with the other hyperparameters first.\n",
    "\n",
    "# begin with max depth and min_child_weight . Comment out the rest on space and keep them stock on xgbclassifier\n",
    "\n",
    "\n",
    "#sample model to fire up the gpu so we can measure time elapsed more accurately\n",
    "fire_up = XGBClassifier(n_estimators = 2000,tree_method = \"hist\",device=\"cuda\")\n",
    "fire_up.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Define the objective function for Hyperopt\n",
    "def objective(space):\n",
    "    # Create the XGBoost model with the specified hyperparameters running on gpu\n",
    "    model = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,\n",
    "                          \n",
    "                          n_estimators =5000, \n",
    "                          max_depth= int(space[\"max_depth\"])\n",
    "                          ,min_child_weight= space[\"min_child_weight\"]\n",
    "                          \n",
    "                          ,learning_rate = 0.1)\n",
    "                          \n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "    # Train and evaluate the model for each fold\n",
    "    scores = []\n",
    "    for i , (train_index, val_index) in enumerate(cv.split(X_train, y_train)):\n",
    "        # Split the data into training and evaluation sets\n",
    "        X_train_check, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_check, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        evaluation = [( X_train_check, y_train_check), ( X_val, y_val)]\n",
    "\n",
    "        # Train the model on the training data and use validation data for early stopping\n",
    "        model.fit(X_train_check, y_train_check,eval_metric=\"mlogloss\",eval_set=evaluation,early_stopping_rounds = 500)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model on the test data\n",
    "        score = f1_score(y_test, y_pred, average=\"micro\")\n",
    "        scores.append(score)\n",
    "\n",
    "    # Return the negative average of the scores\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Define the hyperparameter space for Hyperopt\n",
    "space={  'max_depth': hp.quniform(\"max_depth\", 2, 12, 1),\n",
    "         #'gamma': hp.uniform ('gamma', 0,3),\n",
    "         #'reg_alpha' : hp.quniform('reg_alpha', 5,50,1),\n",
    "         #'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "         #'colsample_bytree' : hp.uniform('colsample_bytree', 0.3,1),\n",
    "         'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "         'n_estimators' : 10000,\n",
    "         #'learning_rate' : hp.uniform('learning_rate', 0.01,0.75),\n",
    "         #'subsample' : hp.uniform('subsample', 0.2,1),\n",
    "         \n",
    "     }\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "\n",
    "start = time.time()\n",
    "trials = hyperopt.Trials()\n",
    "best_params = hyperopt.fmin(\n",
    "    objective,\n",
    "    space,\n",
    "    algo=hyperopt.rand.suggest,\n",
    "    max_evals=30,\n",
    "    trials=trials,\n",
    "    verbose=2\n",
    ")\n",
    "end=time.time()\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(best_params)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee79073",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_so_far = []\n",
    "time_elapsed = []\n",
    "\n",
    "best_loss_so_far.append(0.7942)\n",
    "time_elapsed.append(8511.638578891754)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepwise Hyperopt tuning of xgboost with nested crossvalidation\n",
    "\n",
    "#Keep n iterations constant and learning rate at 0.1 and experiment with the other hyperparameters first.\n",
    "\n",
    "# Now update the previous values and follow with subsample and colsample bytree.\n",
    "\n",
    "\n",
    "#sample model to fire up the gpu so we can measure time elapsed more accurately\n",
    "fire_up = XGBClassifier(n_estimators = 2000,tree_method = \"hist\",device=\"cuda\")\n",
    "fire_up.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Define the objective function for Hyperopt\n",
    "def objective(space):\n",
    "    # Create the XGBoost model with the specified hyperparameters running on gpu\n",
    "    model = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,random_state = 45,\n",
    "                          \n",
    "                          n_estimators =5000, \n",
    "                          max_depth= 6\n",
    "                          ,min_child_weight= 7.0\n",
    "                          ,subsample =space[\"subsample\"]\n",
    "                          ,colsample_bytree = space[\"colsample_bytree\"]\n",
    "                          ,learning_rate = 0.1)\n",
    "                          \n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "    # Train and evaluate the model for each fold\n",
    "    scores = []\n",
    "    for i , (train_index, val_index) in enumerate(cv.split(X_train, y_train)):\n",
    "        # Split the data into training and evaluation sets\n",
    "        X_train_check, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_check, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        evaluation = [( X_train_check, y_train_check), ( X_val, y_val)]\n",
    "\n",
    "        # Train the model on the training data and use validation data for early stopping\n",
    "        model.fit(X_train_check, y_train_check,eval_metric=\"mlogloss\",eval_set=evaluation,early_stopping_rounds = 500)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model on the test data\n",
    "        score = f1_score(y_test, y_pred, average=\"micro\")\n",
    "        scores.append(score)\n",
    "\n",
    "    # Return the negative average of the scores\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Define the hyperparameter space for Hyperopt\n",
    "space={  #'max_depth': hp.quniform(\"max_depth\", 2, 12, 1),\n",
    "         #'gamma': hp.uniform ('gamma', 0,3),\n",
    "         #'reg_alpha' : hp.quniform('reg_alpha', 5,50,1),\n",
    "         #'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "         'colsample_bytree' : hp.uniform('colsample_bytree', 0.3,1),\n",
    "         #'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "         'n_estimators' : 10000,\n",
    "         #'learning_rate' : hp.uniform('learning_rate', 0.01,0.75),\n",
    "         'subsample' : hp.uniform('subsample', 0.2,1),\n",
    "         \n",
    "     }\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "\n",
    "start = time.time()\n",
    "trials = hyperopt.Trials()\n",
    "best_params = hyperopt.fmin(\n",
    "    objective,\n",
    "    space,\n",
    "    algo=hyperopt.rand.suggest,\n",
    "    max_evals=30,\n",
    "    trials=trials,\n",
    "    verbose=2\n",
    ")\n",
    "end=time.time()\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(best_params)\n",
    "time_elapsed.append(end-start)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efae9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_so_far.append(0.80560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepwise Hyperopt tuning of xgboost with nested crossvalidation\n",
    "\n",
    "#Keep n iterations constant and learning rate at 0.1 and experiment with the other hyperparameters first.\n",
    "\n",
    "# Now update the previous values and follow with learning rate.\n",
    "\n",
    "\n",
    "#sample model to fire up the gpu so we can measure time elapsed more accurately\n",
    "fire_up = XGBClassifier(n_estimators = 2000,tree_method = \"hist\",device=\"cuda\")\n",
    "fire_up.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Define the objective function for Hyperopt\n",
    "def objective(space):\n",
    "    # Create the XGBoost model with the specified hyperparameters running on gpu\n",
    "    model = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,random_state = 45,\n",
    "                          \n",
    "                          n_estimators =5000, \n",
    "                          max_depth= 6\n",
    "                          ,min_child_weight= 7.0\n",
    "                          ,subsample =0.4942336627346027\n",
    "                          ,colsample_bytree = 0.3878027876119541\n",
    "                          ,learning_rate = space[\"learning_rate\"])\n",
    "                          \n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "    # Train and evaluate the model for each fold\n",
    "    scores = []\n",
    "    for i , (train_index, val_index) in enumerate(cv.split(X_train, y_train)):\n",
    "        # Split the data into training and evaluation sets\n",
    "        X_train_check, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_check, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        evaluation = [( X_train_check, y_train_check), ( X_val, y_val)]\n",
    "\n",
    "        # Train the model on the training data and use validation data for early stopping\n",
    "        model.fit(X_train_check, y_train_check,eval_metric=\"mlogloss\",eval_set=evaluation,early_stopping_rounds = 500)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model on the test data\n",
    "        score = f1_score(y_test, y_pred, average=\"micro\")\n",
    "        scores.append(score)\n",
    "\n",
    "    # Return the negative average of the scores\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Define the hyperparameter space for Hyperopt\n",
    "space={  #'max_depth': hp.quniform(\"max_depth\", 2, 12, 1),\n",
    "         #'gamma': hp.uniform ('gamma', 0,3),\n",
    "         #'reg_alpha' : hp.quniform('reg_alpha', 5,50,1),\n",
    "         #'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "         #'colsample_bytree' : hp.uniform('colsample_bytree', 0.3,1),\n",
    "         #'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "         'n_estimators' : 10000,\n",
    "         'learning_rate' : hp.uniform('learning_rate', 0.01,0.75),\n",
    "         #'subsample' : hp.uniform('subsample', 0.2,1),\n",
    "         \n",
    "     }\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "\n",
    "start = time.time()\n",
    "trials = hyperopt.Trials()\n",
    "best_params = hyperopt.fmin(\n",
    "    objective,\n",
    "    space,\n",
    "    algo=hyperopt.rand.suggest,\n",
    "    max_evals=30,\n",
    "    trials=trials,\n",
    "    verbose=2\n",
    ")\n",
    "end=time.time()\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(best_params)\n",
    "time_elapsed.append(end-start)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25679045",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_so_far.append(0.80979)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepwise Hyperopt tuning of xgboost with nested crossvalidation\n",
    "\n",
    "#Keep n iterations constant and learning rate at 0.1 and experiment with the other hyperparameters first.\n",
    "\n",
    "# Now update the previous values and follow with learning rate.\n",
    "\n",
    "\n",
    "#sample model to fire up the gpu so we can measure time elapsed more accurately\n",
    "fire_up = XGBClassifier(n_estimators = 2000,tree_method = \"hist\",device=\"cuda\")\n",
    "fire_up.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Define the objective function for Hyperopt\n",
    "def objective(space):\n",
    "    # Create the XGBoost model with the specified hyperparameters running on gpu\n",
    "    model = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,random_state = 45,\n",
    "                          \n",
    "                          n_estimators =1000, \n",
    "                          max_depth= 6\n",
    "                          ,min_child_weight= 7.0\n",
    "                          ,subsample =0.4942336627346027\n",
    "                          ,colsample_bytree = 0.3878027876119541\n",
    "                          ,learning_rate = 0.029058263960152986\n",
    "                          ,gamma = space[\"gamma\"])\n",
    "                          \n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    \n",
    "\n",
    "    # Train and evaluate the model for each fold\n",
    "    scores = []\n",
    "    for i , (train_index, val_index) in enumerate(cv.split(X_train, y_train)):\n",
    "        # Split the data into training and evaluation sets\n",
    "        X_train_check, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_check, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        evaluation = [( X_train_check, y_train_check), ( X_val, y_val)]\n",
    "\n",
    "        # Train the model on the training data and use validation data for early stopping\n",
    "        model.fit(X_train_check, y_train_check,eval_metric=\"mlogloss\",eval_set=evaluation,early_stopping_rounds = 50)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model on the test data\n",
    "        score = f1_score(y_test, y_pred, average=\"micro\")\n",
    "        scores.append(score)\n",
    "\n",
    "    # Return the negative average of the scores\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Define the hyperparameter space for Hyperopt\n",
    "space={  #'max_depth': hp.quniform(\"max_depth\", 2, 12, 1),\n",
    "         'gamma': hp.uniform ('gamma', 0,3),\n",
    "         #'reg_alpha' : hp.quniform('reg_alpha', 5,50,1),\n",
    "         #'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "         #'colsample_bytree' : hp.uniform('colsample_bytree', 0.3,1),\n",
    "         #'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "         'n_estimators' : 10000,\n",
    "         #'learning_rate' : hp.uniform('learning_rate', 0.01,0.75),\n",
    "         #'subsample' : hp.uniform('subsample', 0.2,1),\n",
    "         \n",
    "     }\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "\n",
    "start = time.time()\n",
    "trials = hyperopt.Trials()\n",
    "best_params = hyperopt.fmin(\n",
    "    objective,\n",
    "    space,\n",
    "    algo=hyperopt.rand.suggest,\n",
    "    max_evals=30,\n",
    "    trials=trials,\n",
    "    verbose=2\n",
    ")\n",
    "end=time.time()\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(best_params)\n",
    "time_elapsed.append(end-start)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4742e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_so_far.append(0.80840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time passed doing training in hours\n",
    "\n",
    "np.array(time_elapsed).sum()/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "g = sns.lineplot(data=best_loss_so_far,marker=\"o\",markersize = 12)\n",
    "g.set_xticks(range(len(best_loss_so_far)))\n",
    "#g.set_xticklabels(np.arange(1,16),fontsize = 12)\n",
    "plt.title(\"Accuracy gains or losses per step\",fontsize = 20)\n",
    "\n",
    "steps = [\"Max depth\\nMinChildWeight\",\"Subsample\\nColsampleBytree\",\"LearningRate\",\"Gamma\"]\n",
    "for i in range(4) :\n",
    "\n",
    "    g.text(i+0.1,best_loss_so_far[i]-0.0002,steps[i])\n",
    "    \n",
    "plt.xlabel(\"Steps\",fontsize=15)\n",
    "plt.ylabel(\"Accuracy\",fontsize=15)\n",
    "    \n",
    "plt.savefig(\"C:/Users/alexk/Desktop/protipa/highlowaccuracy4\", bbox_inches='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e64e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain without gamma on whole model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_without_gamma = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,random_state = 45,\n",
    "                          \n",
    "                          n_estimators =10000, \n",
    "                          max_depth= 6\n",
    "                          ,min_child_weight= 7.0\n",
    "                          ,subsample =0.4942336627346027\n",
    "                          ,colsample_bytree = 0.3878027876119541\n",
    "                          ,learning_rate = 0.029058263960152986\n",
    "                          )\n",
    "\n",
    "evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "\n",
    "xgb_without_gamma.fit(X_train, y_train,eval_metric=\"mlogloss\",eval_set=evaluation,early_stopping_rounds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76002a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nogamma = xgb_without_gamma.predict(X_test)\n",
    "\n",
    "# compute and print accuracy score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('XGBoost model accuracy score: {}'.format(accuracy_score(y_test, y_pred_nogamma)))\n",
    "print('XGBoost model precision score: {}'.format(precision_score(y_test, y_pred_nogamma,average=\"weighted\")))\n",
    "print('XGBoost model recall score: {}'.format(recall_score(y_test, y_pred_nogamma,average=\"weighted\")))\n",
    "print('XGBoost model f1 score: {}'.format(f1_score(y_test, y_pred_nogamma,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try to oversample the imbalanced classes\n",
    "\n",
    "smenn = SMOTETomek(random_state = 56)\n",
    "\n",
    "X_train_resample,y_train_resample = smenn.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02433d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_resampled = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,random_state = 45,\n",
    "                          \n",
    "                          n_estimators =10000, \n",
    "                          max_depth= 6\n",
    "                          ,min_child_weight= 7.0\n",
    "                          ,subsample =0.4942336627346027\n",
    "                          ,colsample_bytree = 0.3878027876119541\n",
    "                          ,learning_rate = 0.029058263960152986\n",
    "                          )\n",
    "\n",
    "evaluation = [( X_train_resample, y_train_resample), ( X_test, y_test)]\n",
    "\n",
    "xgb_resampled.fit(X_train_resample, y_train_resample,eval_metric=\"mlogloss\",eval_set=evaluation,early_stopping_rounds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_res = xgb_resampled.predict(X_test)\n",
    "\n",
    "# compute and print accuracy score\n",
    "#Slight Accuracy increase when balancing out the classes (shoul)\n",
    "\n",
    "print('XGBoost-upsampled model accuracy score: {}'.format(accuracy_score(y_test, y_pred_res)))\n",
    "print('XGBoost-upsampled model precision score: {}'.format(precision_score(y_test, y_pred_res,average=\"weighted\")))\n",
    "print('XGBoost-upsampled model recall score: {}'.format(recall_score(y_test, y_pred_res,average=\"weighted\")))\n",
    "print('XGBoost-upsampled model f1 score: {}'.format(f1_score(y_test, y_pred_res,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a confusion matrix for the best model which is xgboost with upsampling\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred_res)\n",
    "\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = [\"0\",\"1\",\"2\",\"3\",\"4\"], \n",
    "                     columns = [\"0\",\"1\",\"2\",\"3\",\"4\"])\n",
    "\n",
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix',fontsize=20)\n",
    "plt.ylabel('Actal Values',fontsize=12)\n",
    "plt.xlabel('Predicted Values',fontsize=12)\n",
    "\n",
    "plt.savefig(\"C:/Users/alexk/Desktop/protipa/confusionmatrix\", bbox_inches='tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3505baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final model will be retrained on the whole dataset (X,y (no split)) and predict on the external DatasetC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction_df = pd.read_csv ('C:/Users/alexk/Downloads/datasetCtest.csv',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use value around these (if they are close) for iterations of final model\n",
    "\n",
    "print(xgb_without_gamma.best_iteration,xgb_resampled.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train final model on whole X y upsampled\n",
    "\n",
    "smenn = SMOTETomek(random_state = 56)\n",
    "\n",
    "X_train_resample_full,y_train_resample_full = smenn.fit_resample(X,y)\n",
    "\n",
    "\n",
    "xgb_final = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,random_state = 45,\n",
    "                          \n",
    "                          n_estimators =1930, \n",
    "                          max_depth= 6\n",
    "                          ,min_child_weight= 7.0\n",
    "                          ,subsample =0.4942336627346027\n",
    "                          ,colsample_bytree = 0.3878027876119541\n",
    "                          ,learning_rate = 0.029058263960152986\n",
    "                          )\n",
    "\n",
    "xgb_final.fit(X_train_resample_full,y_train_resample_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274812ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = xgb_final.predict(final_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c194c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change from 0-1-2-3-4 to 1-2-3-4-5\n",
    "\n",
    "final_predictions = np.array(final_predictions) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07626b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"C:/Users/alexk/Desktop/protipa/labels27\",final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88227206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#double check if everything works\n",
    "\n",
    "check_preds = np.load(\"C:/Users/alexk/Desktop/protipa/labels27.npy\")\n",
    "\n",
    "check_preds.shape\n",
    "\n",
    "if check_preds.all() == final_predictions.all() :\n",
    "    \n",
    "    print(\"All good loaded array shape is {}\".format(check_preds.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Now we will try to do a 2d decision boundary plot like previous exercises. We will create a 2d embedding of the 400 features using UMAP\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#reducer = UMAP(n_neighbors=100,n_components=2,random_state=55)\n",
    "\n",
    "X_train_embedding = pca.fit_transform(X_train,y = y_train)\n",
    "\n",
    "# Never fit the test data only transform to not cause data leakage !!!\n",
    "X_test_embedding = pca.transform(X_test)\n",
    "\n",
    "xgb_umap = XGBClassifier(objective=\"multi:huber\",tree_method = \"hist\",device=\"cuda\",n_classes = 5,random_state = 45,\n",
    "                          \n",
    "                          n_estimators =10000, \n",
    "                          max_depth= 6\n",
    "                          ,min_child_weight= 7.0\n",
    "                          ,subsample =0.4942336627346027\n",
    "                          ,colsample_bytree = 0.3878027876119541\n",
    "                          ,learning_rate = 0.029058263960152986\n",
    "                          )\n",
    "\n",
    "evaluation = [( X_train_embedding, y_train), ( X_test_embedding, y_test)]\n",
    "\n",
    "xgb_umap.fit(X_train_embedding, y_train,eval_metric=\"mlogloss\",eval_set=evaluation,early_stopping_rounds = 1000,verbose=False)\n",
    "\n",
    "umap_predss = xgb_umap.predict(X_test_embedding)\n",
    "\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, umap_predss))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "#sns.set(context=\"notebook\", style=\"darkgrid\",rc={\"axes.axisbelow\": False})\n",
    "\n",
    "custom_cmap =  ListedColormap(['red','green','blue','yellow','black','purple'])\n",
    "\n",
    "#sns.set_style=\"darkgrid\"\n",
    "\n",
    "#Use the already fitted model to create the decision surface by making predictions \n",
    "#for a grid of values across the input domain\n",
    "\n",
    "#Find the minimum and maximum values for each feature and expand \n",
    "#the grid one step beyond that to ensure the whole feature space is covered.\n",
    "x_min, x_max = X_test_embedding[:, 0].min() - 1, X_test_embedding[:, 0].max() + 1\n",
    "y_min, y_max = X_test_embedding[:, 1].min() - 1, X_test_embedding[:, 1].max() + 1\n",
    "\n",
    "\n",
    "#Create a uniform sample across each dimension using the linspace() function and make it really fine with 2000 points\n",
    "#Use the meshgrid() function to create a grid from these two vectors.\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max,2000),np.linspace(y_min, y_max,2000))\n",
    "\n",
    "#Transform meshgrid to an expected form for the predict function. Flatten arrays with ravel function.\n",
    "#Create a 2d array from the two 1d arrays using np.c_ function . Then we can use predict without errors.\n",
    "Z = xgb_umap.predict(np.c_[xx.ravel(), yy.ravel()]) \n",
    "\n",
    "#Reshape to the starting meshgrid shape and plot the predicted contour.\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=custom_cmap,alpha = 0.5,zorder = 0)\n",
    "\n",
    "\n",
    "# Seperate right wrong predictions to plot them with a different symbol\n",
    "\n",
    "tp = (y_test == umap_predss)  # True Positive\n",
    "tp0, tp1,tp2,tp3,tp4 = tp[y_test == 0], tp[y_test == 1] ,tp[y_test ==2], tp[y_test == 3] ,tp[y_test ==4]\n",
    "X0, X1,X2,X3,X4 = X_test_embedding[y_test == 0], X_test_embedding[y_test == 1],X_test_embedding[y_test==2],X_test_embedding[y_test == 3],X_test_embedding[y_test==4]\n",
    "X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "X2_tp, X2_fp = X2[tp2], X2[~tp2]\n",
    "X3_tp, X3_fp = X3[tp3], X3[~tp3]\n",
    "X4_tp, X4_fp = X4[tp4], X4[~tp4]\n",
    "\n",
    "#plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_test,cmap=custom_cmap,edgecolor=\"k\",marker=\"o\")\n",
    "\n",
    "\n",
    "plt.scatter(X0_tp[:, 0], X0_tp[:, 1], c=\"red\",edgecolor=\"k\",marker=\"o\",label = \"0\")\n",
    "plt.scatter(X0_fp[:, 0], X0_fp[:, 1], c=\"red\",marker=\"x\",label = \"0 missed\")\n",
    "\n",
    "plt.scatter(X1_tp[:, 0], X1_tp[:, 1], c=\"green\",edgecolor=\"k\",marker=\"o\",label = \"1\")\n",
    "plt.scatter(X1_fp[:, 0], X1_fp[:, 1], c=\"green\",marker=\"x\",label = \"1 missed\")\n",
    "\n",
    "plt.scatter(X2_tp[:, 0], X2_tp[:, 1], c=\"blue\",edgecolor=\"k\",marker=\"o\",label = \"2\")\n",
    "plt.scatter(X2_fp[:, 0], X2_fp[:, 1], c=\"blue\",marker=\"x\",label = \"2 missed\")\n",
    "\n",
    "plt.scatter(X3_tp[:, 0], X3_tp[:, 1], c=\"black\",edgecolor=\"k\",marker=\"o\",label = \"3\")\n",
    "plt.scatter(X3_fp[:, 0], X3_fp[:, 1], c=\"black\",marker=\"x\",label = \"3 missed\")\n",
    "\n",
    "plt.scatter(X4_tp[:, 0], X4_tp[:, 1], c=\"purple\",edgecolor=\"k\",marker=\"o\",label = \"4\")\n",
    "plt.scatter(X4_fp[:, 0], X4_fp[:, 1], c=\"purple\",marker=\"x\",label = \"4 missed\")\n",
    "\n",
    "\n",
    "\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.plot_surface(xx, yy, Z, cmap='hsv', alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.title(\"Decision Boundary plot of XGB\",fontsize = 20)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(\"C:/Users/alexk/Desktop/protipa/decisionxgboost\", bbox_inches='tight') \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62520e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c081f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167c69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435d28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763fc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f841e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d401d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f83d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f01a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
